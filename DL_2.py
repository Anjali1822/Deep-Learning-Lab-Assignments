# -*- coding: utf-8 -*-
"""DL_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O5Y0GGH55Uc_nGlv0ja6UaDUyeSYVbp9
"""

'''
Implementing Feedforward neural networks with Keras and TensorFlow
a. Import the necessary packages
b. Load the training and testing data (MNIST/CIFAR10)
c. Define the network architecture using Keras
d. Train the model using SGD
e. Evaluate the network
f. Plot the training loss and accuracy
'''

#import necessary packages
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
import random

# Load the training and testing data
data=tf.keras.datasets.mnist     #importing MNIST dataset
from tensorflow.keras.optimizers import SGD
#from tensorflow.keras.datasets import mnist

(x_train,y_train),(x_test,y_test)=data.load_data()   #splitting it into training and testing dataset  # x is attribute and y is classes

x_test.shape        # show the data set of test 28 pixel , 28 pixel   picture

x_train.shape    # show the data set of training   28 pixel , 28 pixel   picture
print(x_train[0])

# To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to a range of values from 0 to 1.
# The simplest way is to divide the value of every pixel by 255 to get the values in the range of 0 to 1

x_train= x_train/255
x_test = x_test/255      # Covert all values between 0 to 1

#Define the network architecture using Keras :
'''
Now we have to build the network => we need input hiddent and output layer
Sequential Model used because we have to create FNN
'''

model=tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=(28,28)),  #covert 2D into 1D:INPUT IMAGE IS CONVERTED INTO VECTOR BEACUSE CNN NETOWRK REQUIRED DATA IN FORM OF VECTOR
tf.keras.layers.Dense(150,activation='relu'),      #  Dense Means Fully connected Network in Hiden Layer
tf.keras.layers.Dense(10,activation='softmax')     # classifying into 10 classes output have TEN neuron
])

model.summary()

#Train the model using SGD (Stochastic Gradient Descent)

# The default learning rate is 0.01 and no momentum is used by default.
sgd=SGD(0.02)    # is learning rate  0.02
#adam
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#loss = measure disimilarity metrics = judge the performance of model

history=model.fit(x_train, y_train,validation_data=(x_test,y_test),epochs=5) # Train the model

# Evaluate the model
test_loss,test_acc=model.evaluate(x_test,y_test)

print("Loss=%.3f"%test_loss)
print("Accuracy=%.3f"%test_acc)

n=random.randint(0,9999)
plt.imshow(x_test[n])
plt.show()

plt.imshow(x_test[4])
prediction=model.predict(x_test)   #predict the data
print(np.argmax(prediction[4]))   # print data depend on max probaiblites
plt.show()

#plotting the training Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model_accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train','Validation'],loc='upper right')
plt.show()

#plotting the training Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model Loss')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['Train','Validation'],loc='upper left')
plt.show()

#Plot the training loss and accuracy
# graph represents the modelâ€™s loss
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('Training Loss and accuracy')
plt.ylabel('accuracy/Loss')
plt.xlabel('epoch')
plt.legend(['accuracy', 'val_accuracy','loss','val_loss'])
plt.show()